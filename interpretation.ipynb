{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_context('talk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.axes._axes import _log as matplotlib_axes_logger\n",
    "matplotlib_axes_logger.setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpret scikit-learn machine learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook uses some features from scikit-learn which are under-development:\n",
    "\n",
    "* DataFrame handling with OpenML datasets: https://github.com/scikit-learn/scikit-learn/pull/13902\n",
    "* Fast partial dependence plot for Gradient Boosting Decsision Trees: https://github.com/scikit-learn/scikit-learn/pull/13769\n",
    "* Permutation feature importance: https://github.com/scikit-learn/scikit-learn/pull/13146\n",
    "\n",
    "These features have been combined into a scikit-learn branch in the following repository: https://github.com/glemaitre/scikit-learn/tree/workshop\n",
    "\n",
    "You can refer to the following documentation to install scikit-learn from such source: https://scikit-learn.org/stable/developers/advanced_installation.html#install-bleeding-edge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset: Current Population Survey (1985)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use data from the \"Current Population Survey\" from 1985 and fetch it from [OpenML](http://openml.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "survey = fetch_openml(data_id=534, return_frame=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get more information regarding by looking at the description of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(survey.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data are stored in a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey.data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column **WAGE** is our target variable (i.e., the variable which we want to predict). You can note that the dataset contains both numerical and categorical data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's get some insights by looking at the **marginal** links between the different variables. Only *numerical* variables will be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(survey.data, diag_kind='kde');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add some additional information to the previous plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.PairGrid(survey.data)\n",
    "g = g.map_upper(\n",
    "    sns.regplot, scatter_kws={\"color\": \"black\", \"alpha\": 0.2, \"s\": 30},\n",
    "    line_kws={\"color\": \"red\"}\n",
    ")\n",
    "g = g.map_lower(sns.kdeplot, cmap=\"Reds_d\")\n",
    "g = g.map_diag(sns.kdeplot, shade=True, color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can already have some intuitions regarding our dataset:\n",
    "\n",
    "* The \"WAGE\" distribution has a long tail and we could work by taking the `log` of the wage;\n",
    "* For all 3 variables, \"EDUCATION\", \"EXPERIENCE\", and \"AGE\", the \"WAGE\" is increasing when these variables are increasing;\n",
    "* The \"EXPERIENCE\" and \"AGE\" are correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to point out that we analyzed the data by looking at the joint distribution between 2 variables which is a **marginal** link.\n",
    "\n",
    "We will shortly jump into interpreting the coefficients of linear model. In this regard, we should emphasize that linear models compute **conditional** links. All interpretation of the value coefficient given the relationship between the feature and the target given that other features remain constant. For instance, we can deduce the relationship between the \"AGE\" and \"WAGE\" for a given number of year of \"EXPERIENCE\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpret coefficients of linear models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = survey.data[survey.feature_names]\n",
    "y = survey.data[survey.target_names].values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have heterogeneous data and we will need to make a specific preprocessing for each data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "categorical_columns = ['RACE', 'OCCUPATION', 'SECTOR']\n",
    "binary_columns = ['MARR', 'UNION', 'SEX', 'SOUTH']\n",
    "numerical_columns = ['EDUCATION', 'EXPERIENCE', 'AGE']\n",
    "\n",
    "preprocessor = make_column_transformer(\n",
    "    (OneHotEncoder(), categorical_columns),\n",
    "    (OrdinalEncoder(), binary_columns),\n",
    "    remainder='passthrough'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, the `preprocessor` will:\n",
    "\n",
    "* one-hot encode (i.e., generate a column by category) the categorical columns;\n",
    "* replace by 0 and 1 the categories of binary columns;\n",
    "* keep numerical values as they are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will fit a ridge regressor and transform the target before the fit using a log transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "\n",
    "model = make_pipeline(\n",
    "    preprocessor,\n",
    "    TransformedTargetRegressor(\n",
    "        regressor=RidgeCV(),\n",
    "        func=np.log10,\n",
    "        inverse_func=sp.special.exp10\n",
    "    )\n",
    ")\n",
    "model.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the performance of the model which fitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import median_absolute_error\n",
    "\n",
    "def mae_scorer(model, X_train, X_test, y_train, y_test):\n",
    "    y_pred = model.predict(X_train)\n",
    "    string_score = f'MAE on training set: {median_absolute_error(y_train, y_pred):.2f} $/hour'\n",
    "    y_pred = model.predict(X_test)\n",
    "    string_score += f'\\nMAE on testing set: {median_absolute_error(y_test, y_pred):.2f} $/hour'\n",
    "    return string_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 6))\n",
    "y_pred = model.predict(X_test)\n",
    "sns.regplot(y_test, y_pred)\n",
    "\n",
    "plt.text(3, 20, mae_scorer(model, X_train, X_test, y_train, y_test))\n",
    "\n",
    "plt.ylabel('Model preditions')\n",
    "plt.xlabel('Truths')\n",
    "plt.xlim([0, 27])\n",
    "plt.ylim([0, 27]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model learnt is far to be a good model making accurate prediction. Interpretation tools are characterizing model rather than the generative process of the data itself. Thus, interpretations are correct if the model is correct as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now, we can plot the values of the coefficients of the regressor which we fitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = (model.named_steps['columntransformer']\n",
    "                      .named_transformers_['onehotencoder']\n",
    "                      .get_feature_names(input_features=categorical_columns))\n",
    "feature_names = np.concatenate([feature_names, binary_columns, numerical_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "coefs = pd.DataFrame(\n",
    "    model.named_steps['transformedtargetregressor'].regressor_.coef_,\n",
    "    columns=['Coefficients'], index=feature_names\n",
    ")\n",
    "coefs.plot(kind='barh', figsize=(9, 7))\n",
    "plt.axvline(x=0, color='.5');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One limitation is that we cannot compare the different weights since we did not scale the data during fit and that features can have different range. For instance, the \"AGE\" coefficient is expressed in `$/hours/leaving years` while the \"EDUCATION\" is expressed in `$/hours/years of education`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_preprocessed = pd.DataFrame(\n",
    "    model.named_steps['columntransformer'].transform(X_train),\n",
    "    columns=feature_names\n",
    ")\n",
    "X_train_preprocessed.std().plot(kind='barh', figsize=(9, 7))\n",
    "plt.title('Features std. dev.');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can normalize the weights by the standard deviation and then we will be able to compare the different weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = pd.DataFrame(\n",
    "    model.named_steps['transformedtargetregressor'].regressor_.coef_ *\n",
    "    X_train_preprocessed.std(),\n",
    "    columns=['Coefficients'], index=feature_names\n",
    ")\n",
    "coefs.plot(kind='barh', figsize=(9, 7))\n",
    "plt.axvline(x=0, color='.5');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way that we can interpret these values is as follow. An increase of the \"AGE\" will induce a decrease of the \"WAGE\" when all other features remain constant or an increase of the \"EXPERIENCE\" will induce an increase of the \"WAGE\" when all other features remain constant.\n",
    "\n",
    "The first interpretation might look counter-intuitive at first, if one relates the relationship between \"AGE\" and \"WAGE\" as a *marginal* link. However, as previously mentioned, a linear model computes a *conditional* link between \"AGE\" and \"WAGE\" given all other features.\n",
    "\n",
    "Therefore, one could interpret that for a given experience (and all other features constant as well ...), a younger person would have an higher wage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to now, we did not check the stability of the coefficients and it would be important to check it to ensure through cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "cv_model = cross_validate(\n",
    "    model, X, y, cv=RepeatedKFold(n_splits=5, n_repeats=5),\n",
    "    return_estimator=True, n_jobs=-1\n",
    ")\n",
    "coefs = pd.DataFrame(\n",
    "    [est.named_steps['transformedtargetregressor'].regressor_.coef_ *\n",
    "     X_train_preprocessed.std()\n",
    "     for est in cv_model['estimator']],\n",
    "    columns=feature_names\n",
    ")\n",
    "plt.figure(figsize=(9, 7))\n",
    "sns.swarmplot(data=coefs, orient='h', color='k', alpha=0.5)\n",
    "sns.boxenplot(data=coefs, orient='h', color='C0')\n",
    "plt.axvline(x=0, color='.5')\n",
    "plt.title('Stability of coefficients');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"AGE\" and \"EXPERIENCE\" are highly instable which might be due to the collinearity between the 2 features. We can remove on of the 2 features and check what is the impact on the features stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_to_drop = ['AGE']\n",
    "\n",
    "cv_model = cross_validate(\n",
    "    model, X.drop(columns=column_to_drop), y,\n",
    "    cv=RepeatedKFold(n_splits=5, n_repeats=5),\n",
    "    return_estimator=True, n_jobs=-1\n",
    ")\n",
    "coefs = pd.DataFrame(\n",
    "    [est.named_steps['transformedtargetregressor'].regressor_.coef_ *\n",
    "     X_train_preprocessed.drop(columns=column_to_drop).std()\n",
    "     for est in cv_model['estimator']],\n",
    "    columns=feature_names[:-1]\n",
    ")\n",
    "plt.figure(figsize=(9, 7))\n",
    "sns.swarmplot(data=coefs, orient='h', color='k', alpha=0.5)\n",
    "sns.boxenplot(data=coefs, orient='h', color='C0')\n",
    "plt.axvline(x=0, color='.5')\n",
    "plt.title('Stability of coefficients');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With real-life dataset, data are leaving in a high-dimensional space where features are correlated and to make thing more difficult, we can have a limited number of samples. In this condition, estimating the coefficients is really difficult and we use regularization.\n",
    "\n",
    "Ridge implements a $l_2$ regularization and we could use instead lasso which uses $l_1$. Lasso will select a subgroup of variable. However, it will be instable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "model = make_pipeline(\n",
    "    preprocessor,\n",
    "    TransformedTargetRegressor(\n",
    "        regressor=LassoCV(max_iter=10000, cv=5),\n",
    "        func=np.log10,\n",
    "        inverse_func=sp.special.exp10\n",
    "    )\n",
    ")\n",
    "model.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = pd.DataFrame(\n",
    "    model.named_steps['transformedtargetregressor'].regressor_.coef_ *\n",
    "    X_train_preprocessed.std(),\n",
    "    columns=['Coefficients'], index=feature_names\n",
    ")\n",
    "coefs.plot(kind='barh', figsize=(9, 7))\n",
    "plt.axvline(x=0, color='.5');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observed that some of the variables have been dropped. We can now check the stability of the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_model = cross_validate(\n",
    "    model, X, y, cv=RepeatedKFold(n_splits=5, n_repeats=5),\n",
    "    return_estimator=True, n_jobs=-1\n",
    ")\n",
    "coefs = pd.DataFrame(\n",
    "    [est.named_steps['transformedtargetregressor'].regressor_.coef_ *\n",
    "     X_train_preprocessed.std()\n",
    "     for est in cv_model['estimator']],\n",
    "    columns=feature_names\n",
    ")\n",
    "plt.figure(figsize=(9, 7))\n",
    "sns.swarmplot(data=coefs, orient='h', color='k', alpha=0.5)\n",
    "sns.boxenplot(data=coefs, orient='h', color='C0')\n",
    "plt.axvline(x=0, color='.5')\n",
    "plt.title('Stability of coefficients');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take-home message\n",
    "\n",
    "* linear model coefficients represent conditional link between a variable and the target;\n",
    "* coefficients cannot be compared if they were not normalized;\n",
    "* regularization will have to be used when a problem starts to be complex (e.g. with a large number of features);\n",
    "* collinearity makes interpretation difficult;\n",
    "* interpretations characterize the fitted model rather than the process that generated the data (especially if the model assumptions do not hold)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpret tree-based models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following section, we will focus on the so-called \"feature importance\". Feature importance are primilarly known for the tree-based algorithms. It is based on the fraction of samples a feature contributes to combined with the decrease in impurity from splitting them. However, we will see that this implementation suffers from some bias which we will highlight. To highlight the drawback of the random forest feature importances, we will add 2 random features (1 categorical feature and 1 numerical feature)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = survey.data[survey.feature_names]\n",
    "y = survey.data[survey.target_names].values.ravel()\n",
    "\n",
    "X['random_cat'] = np.random.randint(3, size=X.shape[0])\n",
    "X['random_num'] = np.random.randn(X.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest feature importances of an overfitted model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will fit a random forest model. On purpose, we will grow each tree in the forest until each sample will be in a leaf. It implies that each tree will overfit. It allows us to highlight the issues of the feature importance computed in random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "categorical_columns = ['RACE', 'OCCUPATION', 'SECTOR', 'random_cat']\n",
    "binary_columns = ['MARR', 'UNION', 'SEX', 'SOUTH']\n",
    "numerical_columns = ['EDUCATION', 'EXPERIENCE', 'AGE', 'random_num']\n",
    "\n",
    "preprocessor = make_column_transformer(\n",
    "    (OneHotEncoder(), categorical_columns),\n",
    "    (OrdinalEncoder(), binary_columns),\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "model = make_pipeline(\n",
    "    preprocessor,\n",
    "    RandomForestRegressor(min_samples_leaf=1, random_state=42)\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "print(mae_scorer(model, X_train, X_test, y_train, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are getting a very high training compared to the testing score which is synonymous of having a model which overfits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = (model.named_steps['columntransformer']\n",
    "                      .named_transformers_['onehotencoder']\n",
    "                      .get_feature_names(input_features=categorical_columns))\n",
    "feature_names = np.concatenate([feature_names, binary_columns, numerical_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_feature_importances = pd.DataFrame(\n",
    "    model.named_steps['randomforestregressor'].feature_importances_,\n",
    "    index=feature_names,\n",
    "    columns=['RF Feature Importances']\n",
    ")\n",
    "(tree_feature_importances.sort_values(by='RF Feature Importances')\n",
    "                         .plot(kind='barh', figsize=(9, 7)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random forest feature importance reports that \"random_num\" is one of the most important features. This result is surprising since this feature does not have any predictive power.\n",
    "\n",
    "We can also notice that the numerical features (random or not) are often much more important than any of the categorical features (random or not). This behavior is due to the bias linked with the cardinality of the unique values in a feature. Indeed, an higher cardinality induces that you have more chance to use this feature to make a split within the tree. Therefore, a typically high-cardinality numerical feature will have a higher importance than a low-cardinality categorical feature.\n",
    "\n",
    "Secondly, a random feature will appear important when a model is overfitted as RF feature importances are computed on training set statistics. They only reflect the use of the features to split training samples, not necessarily their usefulness to make decisions that can generalize to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutation importances on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of using the random feature importance, one should use the \"permutation importance\" which is a different way to characterize the importance of feature using random permutation. To know the predictive power of a feature, we can permutate the value for a single feature and compute the decrease in accuracy. If the feature is a predictive feature then the decrease of accuracy will be higher. We can repeat the permutation several time to have an estiamate of the variance of the importances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "X_test_preprocessed = (model.named_steps['columntransformer']\n",
    "                            .transform(X_test))\n",
    "\n",
    "permute_importance = pd.DataFrame(\n",
    "    permutation_importance(model.named_steps['randomforestregressor'],\n",
    "                           X_test_preprocessed, y_test, n_rounds=30),\n",
    "    index=feature_names\n",
    ")\n",
    "sorted_idx = (permute_importance.mean(axis=1)\n",
    "                                .sort_values(ascending=False)\n",
    "                                .index)\n",
    "\n",
    "plt.figure(figsize=(9, 8))\n",
    "sns.boxplot(data=permute_importance.loc[sorted_idx].T,\n",
    "            orient='h', color='C0')\n",
    "sns.swarmplot(data=permute_importance.loc[sorted_idx].T,\n",
    "              orient='h', color='k', alpha=0.3)\n",
    "plt.axvline(x=0, color='.5')\n",
    "plt.title(\"Permutation Importances (test set)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computing the permutation feature importance, we see that the random feature are not ranking in the top feature anymore. However, we can see that \"EXPERIENCE\" is on the bottom of the ranking. It is due to the collinearity with the \"AGE\" feature. We will see later on what would be the impact on removing one of the correlated feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutation importances on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_preprocessed = (model.named_steps['columntransformer']\n",
    "                             .transform(X_train))\n",
    "\n",
    "permute_importance = pd.DataFrame(\n",
    "    permutation_importance(model.named_steps['randomforestregressor'],\n",
    "                           X_train_preprocessed, y_train, n_rounds=30),\n",
    "    index=feature_names\n",
    ")\n",
    "sorted_idx = (permute_importance.mean(axis=1)\n",
    "                                .sort_values(ascending=False)\n",
    "                                .index)\n",
    "\n",
    "plt.figure(figsize=(9, 8))\n",
    "sns.boxplot(data=permute_importance.loc[sorted_idx].T,\n",
    "            orient='h', color='C0')\n",
    "sns.swarmplot(data=permute_importance.loc[sorted_idx].T,\n",
    "              orient='h', color='k', alpha=0.3)\n",
    "plt.axvline(x=0, color='.5')\n",
    "plt.title(\"Permutation Importances (train set)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same analysis on a non-overfitted model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous example, we made the random forest overfit on the training data on purpose to exacerbate the behavior of the feature importance. We can now fit a model which should less overfit by increasing the number of samples required to create a leaf in the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_params(randomforestregressor__min_samples_leaf=10)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(mae_scorer(model, X_train, X_test, y_train, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training accuracy decreased dratiscally and our model is not overfitting as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_feature_importances = pd.DataFrame(\n",
    "    model.named_steps['randomforestregressor'].feature_importances_,\n",
    "    index=feature_names,\n",
    "    columns=['RF Feature Importances']\n",
    ")\n",
    "(tree_feature_importances.sort_values(by='RF Feature Importances')\n",
    "                         .plot(kind='barh', figsize=(9, 7)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the random feature does not rank as high as before but are still considered important. we can compute the permutation importance to see if there is a change of behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permute_importance = pd.DataFrame(\n",
    "    permutation_importance(model.named_steps['randomforestregressor'],\n",
    "                           X_test_preprocessed, y_test, n_rounds=30),\n",
    "    index=feature_names\n",
    ")\n",
    "sorted_idx = (permute_importance.mean(axis=1)\n",
    "                                .sort_values(ascending=False)\n",
    "                                .index)\n",
    "\n",
    "plt.figure(figsize=(9, 8))\n",
    "sns.boxplot(data=permute_importance.loc[sorted_idx].T,\n",
    "            orient='h', color='C0')\n",
    "sns.swarmplot(data=permute_importance.loc[sorted_idx].T,\n",
    "              orient='h', color='k', alpha=0.3)\n",
    "plt.axvline(x=0, color='.5')\n",
    "plt.title(\"Permutation Importances (test set)\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permute_importance = pd.DataFrame(\n",
    "    permutation_importance(model.named_steps['randomforestregressor'],\n",
    "                           X_train_preprocessed, y_train, n_rounds=30),\n",
    "    index=feature_names\n",
    ")\n",
    "sorted_idx = (permute_importance.mean(axis=1)\n",
    "                                .sort_values(ascending=False)\n",
    "                                .index)\n",
    "\n",
    "plt.figure(figsize=(9, 8))\n",
    "sns.boxplot(data=permute_importance.loc[sorted_idx].T,\n",
    "            orient='h', color='C0')\n",
    "sns.swarmplot(data=permute_importance.loc[sorted_idx].T,\n",
    "              orient='h', color='k', alpha=0.3)\n",
    "plt.axvline(x=0, color='.5')\n",
    "plt.title(\"Permutation Importances (train set)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the feature importance using the permutation is more stable and really similar to the first example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove collinear variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also saw that we have an issue with the importance of the \"EXPERIENCE\" feature which does not rank high in the importance. We thought that it could be linked to the fact that this feature is correlated with the \"AGE\" feature. We could drop one of the feature and check the impact on the feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_to_drop = ['AGE', 'random_num']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train.drop(columns=column_to_drop), y_train)\n",
    "\n",
    "print(\n",
    "    mae_scorer(model,\n",
    "               X_train.drop(columns=column_to_drop),\n",
    "               X_test.drop(columns=column_to_drop),\n",
    "               y_train, y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_feature_importances = pd.DataFrame(\n",
    "    model.named_steps['randomforestregressor'].feature_importances_,\n",
    "    index=feature_names[:-2],\n",
    "    columns=['RF Feature Importances']\n",
    ")\n",
    "(tree_feature_importances.sort_values(by='RF Feature Importances')\n",
    "                         .plot(kind='barh', figsize=(9, 7)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_preprocessed = (model.named_steps['columntransformer']\n",
    "                            .transform(X_test.drop(columns=column_to_drop)))\n",
    "\n",
    "permute_importance = pd.DataFrame(\n",
    "    permutation_importance(model.named_steps['randomforestregressor'],\n",
    "                           X_test_preprocessed, y_test, n_rounds=30),\n",
    "    index=feature_names[:-2]\n",
    ")\n",
    "sorted_idx = (permute_importance.mean(axis=1)\n",
    "                                .sort_values(ascending=False)\n",
    "                                .index)\n",
    "\n",
    "plt.figure(figsize=(9, 8))\n",
    "sns.boxplot(data=permute_importance.loc[sorted_idx].T,\n",
    "            orient='h', color='C0')\n",
    "sns.swarmplot(data=permute_importance.loc[sorted_idx].T,\n",
    "              orient='h', color='k', alpha=0.3)\n",
    "plt.axvline(x=0, color='.5')\n",
    "plt.title(\"Permutation Importances (test set)\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_preprocessed = (model.named_steps['columntransformer']\n",
    "                             .transform(X_train.drop(columns=column_to_drop)))\n",
    "\n",
    "permute_importance = pd.DataFrame(\n",
    "    permutation_importance(model.named_steps['randomforestregressor'],\n",
    "                           X_train_preprocessed, y_train, n_rounds=30),\n",
    "    index=feature_names[:-2]\n",
    ")\n",
    "sorted_idx = (permute_importance.mean(axis=1)\n",
    "                                .sort_values(ascending=False)\n",
    "                                .index)\n",
    "\n",
    "plt.figure(figsize=(9, 8))\n",
    "sns.boxplot(data=permute_importance.loc[sorted_idx].T,\n",
    "            orient='h', color='C0')\n",
    "sns.swarmplot(data=permute_importance.loc[sorted_idx].T,\n",
    "              orient='h', color='k', alpha=0.3)\n",
    "plt.axvline(x=0, color='.5')\n",
    "plt.title(\"Permutation Importances (train set)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the \"EXPERIENCE\" feature starts to be more important. This behaviour is one of the drawback of the permutation importance. If permuting a feature which is correlated with another, there a chance for the model to use the correlated feature to make the decision. Subsquently, there is no decrease of accuracy hidding the importance of this feature. This is one of the limitation of the permutation feature importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Permutation importance for other type of models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The permutation importance can be computed for every type of model. Let's demonstrate with our original ridge model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(\n",
    "    preprocessor,\n",
    "    TransformedTargetRegressor(\n",
    "        regressor=RidgeCV(),\n",
    "        func=np.log10,\n",
    "        inverse_func=sp.special.exp10\n",
    "    )\n",
    ")\n",
    "model.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_preprocessed = (model.named_steps['columntransformer']\n",
    "                            .transform(X_test))\n",
    "\n",
    "permute_importance = pd.DataFrame(\n",
    "    permutation_importance(model.named_steps['transformedtargetregressor'],\n",
    "                           X_test_preprocessed, y_test, n_rounds=30),\n",
    "    index=feature_names\n",
    ")\n",
    "sorted_idx = (permute_importance.mean(axis=1)\n",
    "                                .sort_values(ascending=False)\n",
    "                                .index)\n",
    "\n",
    "plt.figure(figsize=(9, 8))\n",
    "sns.boxplot(data=permute_importance.loc[sorted_idx].T,\n",
    "            orient='h', color='C0')\n",
    "sns.swarmplot(data=permute_importance.loc[sorted_idx].T,\n",
    "              orient='h', color='k', alpha=0.3)\n",
    "plt.axvline(x=0, color='.5')\n",
    "plt.title(\"Permutation Importances (test set)\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_preprocessed = (model.named_steps['columntransformer']\n",
    "                             .transform(X_train))\n",
    "\n",
    "permute_importance = pd.DataFrame(\n",
    "    permutation_importance(model.named_steps['transformedtargetregressor'],\n",
    "                           X_train_preprocessed, y_train, n_rounds=30),\n",
    "    index=feature_names\n",
    ")\n",
    "sorted_idx = (permute_importance.mean(axis=1)\n",
    "                                .sort_values(ascending=False)\n",
    "                                .index)\n",
    "\n",
    "plt.figure(figsize=(9, 8))\n",
    "sns.boxplot(data=permute_importance.loc[sorted_idx].T,\n",
    "            orient='h', color='C0')\n",
    "sns.swarmplot(data=permute_importance.loc[sorted_idx].T,\n",
    "              orient='h', color='k', alpha=0.3)\n",
    "plt.axvline(x=0, color='.5')\n",
    "plt.title(\"Permutation Importances (train set)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take-home message\n",
    "\n",
    "* random forest feature importances suffer from bias;\n",
    "* permutation importances allievate these issues;\n",
    "* permutation importances will minimize the importance of correlated features;\n",
    "* permutation importances is model agnostic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial Dependence Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By fixing and varying the feature values for all samples, we can compute the average predictions and thus get the marginal relationship between this feature and the predicted target. The obtained plot is known as partial dependence plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train a gradient boosting regressor on the censing data, removing the \"EXPERIENCE\" column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.experimental import enable_hist_gradient_boosting  # noqa\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "X = survey.data[survey.feature_names]\n",
    "y = survey.data[survey.target_names].values.ravel()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=42\n",
    ")\n",
    "\n",
    "categorical_columns = ['RACE', 'OCCUPATION', 'SECTOR']\n",
    "binary_columns = ['MARR', 'UNION', 'SEX', 'SOUTH']\n",
    "numerical_columns = ['EDUCATION', 'AGE']\n",
    "\n",
    "preprocessor = make_column_transformer(\n",
    "    (OneHotEncoder(), categorical_columns),\n",
    "    (OrdinalEncoder(), binary_columns),\n",
    "    (FunctionTransformer(validate=False), numerical_columns)\n",
    ")\n",
    "\n",
    "model = make_pipeline(\n",
    "    preprocessor,\n",
    "    HistGradientBoostingRegressor(max_iter=100, max_leaf_nodes=5,\n",
    "                                  learning_rate=0.1, random_state=1)\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "print(mae_scorer(model, X_train, X_test, y_train, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = (model.named_steps['columntransformer']\n",
    "                      .named_transformers_['onehotencoder']\n",
    "                      .get_feature_names(input_features=categorical_columns))\n",
    "feature_names = np.concatenate([feature_names, binary_columns, numerical_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_preprocessed = (model.named_steps['columntransformer']\n",
    "                             .transform(X_train))\n",
    "X_test_preprocessed = (model.named_steps['columntransformer']\n",
    "                            .transform(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then compute the partial dependence between \"AGE\" against \"WAGE\", \"EDUCATION\" against \"WAGE\", and both \"AGE\" and \"EDUCATION\" against \"WAGE\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import plot_partial_dependence\n",
    "\n",
    "features = [16, 17, (16, 17)]\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "plot_partial_dependence(\n",
    "    model.named_steps['histgradientboostingregressor'],\n",
    "    X_train_preprocessed, features, feature_names=feature_names,\n",
    "    n_jobs=-1, grid_resolution=20, fig=fig\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The left-hand plot, we observe that \"WAGE\" increases with \"EDUCATION\" and that there is a kink point around 14 education years. Regarding \"AGE\", we can see an increase up to 40 years old followed by a plateau.\n",
    "\n",
    "However, this is important to notice that the plots are not smooth and we should be careful to not over-interpret them. It could be a good idea to use bootstrap samples to see the variation of marginal effects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdp_helpers import plot_partial_dependence_bootstrap\n",
    "\n",
    "plot_partial_dependence_bootstrap(model, X_train, y_train, features,\n",
    "                                  feature_names, n_boot=10, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"EDUCATION\" plot looks quite stable while the 2 other plots shows variations. Thus, we should be careful when interpreting these plots. One reason for such variation should be linked to the size of the dataset which is rather small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"adult_censing\" notebook is using a similar dataset with a larger number of sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take-home message\n",
    "\n",
    "* partial dependence plot shows the marginal link between one (or a couple) feature and the predicted target capture by a model;\n",
    "* only a small number of samples can be interpreted;\n",
    "* the features are assumed to be independent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect plots for linear models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X = survey.data[survey.feature_names]\n",
    "y = survey.data[survey.target_names].values.ravel()\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, random_state=42\n",
    ")\n",
    "\n",
    "categorical_columns = ['RACE', 'OCCUPATION', 'SECTOR']\n",
    "binary_columns = ['MARR', 'UNION', 'SEX', 'SOUTH']\n",
    "numerical_columns = ['EDUCATION', 'EXPERIENCE', 'AGE']\n",
    "\n",
    "preprocessor = make_column_transformer(\n",
    "    (OneHotEncoder(), categorical_columns),\n",
    "    (OrdinalEncoder(), binary_columns),\n",
    "    (StandardScaler(), numerical_columns)\n",
    ")\n",
    "\n",
    "model = make_pipeline(\n",
    "    preprocessor,\n",
    "    TransformedTargetRegressor(\n",
    "        regressor=RidgeCV(),\n",
    "        func=np.log10,\n",
    "        inverse_func=sp.special.exp10\n",
    "    )\n",
    ")\n",
    "model.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = (model.named_steps['columntransformer']\n",
    "                      .named_transformers_['onehotencoder']\n",
    "                      .get_feature_names(input_features=categorical_columns))\n",
    "feature_names = np.concatenate([feature_names, binary_columns, numerical_columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the effect of the coefficients on the original data by multiplying the coefficients of the linear model by the original data (preprocessed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_preprocessed = pd.DataFrame(\n",
    "    model.named_steps['columntransformer'].transform(X_train),\n",
    "    columns=feature_names\n",
    ")\n",
    "effect = X_train_preprocessed * model.named_steps['transformedtargetregressor'].regressor_.coef_\n",
    "\n",
    "# For the categorical variable, we can inverse the one-hot encoding since only one of the category will be non-null\n",
    "effect_decoded = {}\n",
    "for cat in categorical_columns:\n",
    "    cat_cols = effect.columns.str.contains(cat)\n",
    "    effect_cat = effect.loc[:, cat_cols]\n",
    "    idx_filter = (effect_cat != 0).values\n",
    "    effect_decoded[cat] = effect_cat.values[idx_filter]\n",
    "# For the other columns, we don't need to inverse any embedding\n",
    "for col in binary_columns + numerical_columns:\n",
    "    effect_decoded[col] = effect[col]\n",
    "effect = pd.DataFrame(effect_decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 7))\n",
    "# subsample for the swarmplot\n",
    "sns.swarmplot(data=effect.sample(100), orient='h', color='k', alpha=0.5)\n",
    "sns.boxenplot(data=effect, orient='h', color='C0')\n",
    "plt.axvline(x=0, color='.5');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The largest contribution to the expected wages is related to the \"EDUCATION\" due to the large variance in the effect plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Interpretable Machine Learning, Christph Molnar, 2019. https://github.com/christophM/interpretable-ml-book\n",
    "\n",
    "[2] Beware Default Random Forest Importances, Terence Parr et al., 2018. https://explained.ai/rf-importance/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
